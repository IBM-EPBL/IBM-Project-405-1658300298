# -*- coding: utf-8 -*-
"""crude-oil (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EX4Cj44bGmLt2f-gK7tzv23qVkYvupb8
"""

#Import the libraries that will be needed in the program.
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import os
import seaborn as sns

import os, types
import pandas as pd
from botocore.client import Config
import ibm_boto3

def __iter__(self): return 0

# @hidden_cell
# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.
# You might want to remove those credentials before you share the notebook.
cos_client = ibm_boto3.client(service_name='s3',
    ibm_api_key_id='aTlCyZRbJNCY4qazYOUGZRu3TtwwY-a-OERmvl8USkui',
    ibm_auth_endpoint="https://iam.cloud.ibm.com/oidc/token",
    config=Config(signature_version='oauth'),
    endpoint_url='https://s3.private.us.cloud-object-storage.appdomain.cloud')

bucket = 'crudeoil-donotdelete-pr-6gegwhwphc398u'
object_key = 'Crude Oil Prices Daily.xlsx'

body = cos_client.get_object(Bucket=bucket,Key=object_key)['Body']

data = pd.read_excel(body.read())
data.head()

#Check whether any null values are there or not if it is present then the following can be done,
#Imputing data using the Imputation method in sklearn.
#Filling NaN values with mean, median, and mode using fillna() method.
#Delete the records
data.tail(10)
data.isnull().any()
data.isnull().sum()
data.dropna(axis=0,inplace=True)
data.isnull().sum()

data['Closing Value'].plot(kind='bar')
plt.title('Variation of oil price over years')

data_oil=data["Closing Value"].reset_index()

data_oil

#Feature scaling is a method used to normalize the range of independent variables or features of data.
# The next step is to scale the crude oil prices between (0, 1) to avoid intensive computation. 
# Common methods include Standardization and Normalization.
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler(feature_range=(0,1))
data_oil=scaler.fit_transform(np.array(data_oil).reshape(-1,1))

data_oil

#Data visualization is where a given data set is presented in a graphical format.
# It helps the detection of patterns, trends, and correlations that might go undetected in text-based data.
# Visualize our data using the Matplotlib and seaborn library.
plt.plot(data_oil)
plt.figure(figsize=(15000000,5))

plt.rcParams("figure.figsize")=(20,6)

#When you are working on a model and you want to train it, you have a dataset.
# But after training, we have to test the model on some test dataset.
# For this, you will need a dataset that is different from the training set you used earlier. 
# But it might not always be possible to have so much data during the development phase.
# In such cases, the solution is to split the dataset into two sets, one for training and the other for testing.
training_size=int(len(data_oil)*0.65)
test_size=len(data_oil)-training_size
train_data,test_data=data_oil[0:training_size,:],data_oil[training_size:,:1]
train_data.shape,test_data.shape
#The size of the train and test data after splitting.
training_size,test_size

# convert an array of values into a dataset matrix
def create_dataset (dataset,time_step=1):
    datax,datay =[],[]
    for i in range(len(dataset)-time_step-1) :
        a=dataset[i:(i+time_step), 0]
        datax.append(a)
        datay.append (dataset[i + time_step, 0])
    return np.array(datax),np.array(datay)

# reshape into Xat, t+1, t+2, t+3 and Y=t+4
time_step = 10
x_train,y_train = create_dataset(train_data, time_step)
x_test,y_test = create_dataset(test_data,time_step)

#Shape of training data.
print(x_train.shape),print(y_train.shape)

#Shape of test data.
print (x_test.shape), print (y_test.shape)

#The data of X_train.
x_train

# reshape input to be [samples, time steps, features] which is required for LSTM.
x_train =x_train.reshape(x_train.shape[0],x_train.shape[1],1)
x_test=x_test.reshape(x_test.shape[0],x_test.shape[1],1)

!pip install ibm_watson_machine_learning

from ibm_watson_machine_learning import APIClient
wml_credentials = {
                    "url": "https://us-south.ml.cloud.ibm.com",
                    "apikey": "FuOVK4NIKK7mntk0VwDBd3gD0p_Rg9Y5P3gIKXFhviha"
}
client = APIClient(wml_credentials)

def guid_from_space_name(client, space_name):
    space = client.spaces.get_details()
    #print (space)
    return(next(item for item in space['resources'] if item['entity']["name"] == space_name)['metadata'] ['id'])

space_uid=guid_from_space_name(client,'models')
print("Space uid=" + space_uid)

client.set.default_space(space_uid)

client.software_specifications.list()

software_spec_uid = client.software_specifications.get_uid_by_name("tensorflow_rt22.1-py3.9")
software_spec_uid

model_details = client.repository.store_model(model = "body.tgz" , meta_props = {
    client.repository.ModelMetaNames.NAME : "Crude_oil", 
    client.repository.ModelMetaNames.TYPE : "tensorflow_rt22.1",
    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: software_spec_uid
})
model_id = client.repository.get_model_id(model_details)

model_id









#Create the stacked LSTM model.
#Import the model building libraries.
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense 
from tensorflow.keras.layers import LSTM

#Initializing the libraries.
models=Sequential()

#Adding LSTM layers.
models.add(LSTM(50, return_sequences=True, input_shape=(10,1))) 
models.add(LSTM(50, return_sequences=True))
models.add(LSTM(50))

#Adding output layers.
models.add(Dense(1))

#Information about the model and it's layers.
models.summary()

#Configuring the learning process.
#Metrics are used to evaluate the performance of your model.
models.compile(loss="mean_squared_error",optimizer="adam")

#Train the model.
# RNN weights are updated every 64 stock prices with a batch size of 64.
models.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=50, batch_size=64, verbose=1)

#Tranform to original from.
train_predict= models.predict(x_train)
train_predict=scaler.inverse_transform(train_predict)
test_predict= models.predict(x_test)
test_predict=scaler.inverse_transform(test_predict)

#Calculate RMSeperformance metrics.
import math
from sklearn.metrics import mean_squared_error
math.sqrt (mean_squared_error(y_train, train_predict))

#Save the model.
from tensorflow.keras.models import load_model
models.save("crude_oil.h5")

!tar -zcvf body.tgz crude_oil.h5

ls -1

##Plotting
#Shift train predictions for plotting.
look_back=10
trainPredictPlot = np.empty_like(data_oil)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict
# shift test predictions for plotting
testPredictPlot = np.empty_like(data_oil)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(train_predict)+(look_back*2)+1:len(data_oil)-1,:] = test_predict
# plot baseline and predictions
plt.plot(scaler.inverse_transform(data_oil))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
plt.show()

#Print length of data.
len(test_data)

#Create the input and reshape it and convert it into list
x_input=test_data[5742: ].reshape (1,-1)
x_input.shape

temp_input=list(x_input)
temp_input=temp_input[0].tolist()

temp_input

#For predicting next 10 days crude oil prices we consider n_steps=10.
#We create the input for prediction, index starting from the date 10 days before the first date in the test dataset.
# Then, reshape the inputs to have only 1 column and predict using model_predict predefined function.
lst_output=[] 
n_steps=10
i=0 
while(i<10):
    if(len(temp_input)>10):
        print (temp_input)
        x_input=np.array(temp_input[1:])
        print("{} day input {}".format (i,x_input))
        x_input=x_input.reshape(1,-1)
        x_input = x_input.reshape((1, n_steps, 1))
        print(x_input)
        yhat = model.predict(x_input, verbose=0)
        print("{} day output {}".format (i,yhat))
        temp_input.extend (yhat [0].tolist())
        temp_input=temp_input[1:]
        print(temp_input)
        lst_output.extend(yhat.tolist())
        i=i+1 
    else:
        x_input = x_input.reshape( (1, n_steps,1))
        yhat = model.predict(x_input, verbose=0)
        print (yhat [0])
        temp_input.extend(yhat[0].tolist())
        print(len(temp_input))
        lst_output.extend (yhat.tolist())
        i=i+1

#Create a visualization plot to easily review the prediction.
day_new=np.arange(1,11)
day_pred=np.arange (11,21)
len(data_oil)
plt.plot(day_new, scaler.inverse_transform(data_oil[16422:])) 
plt.plot(day_pred, scaler.inverse_transform(lst_output))
k=scaler.inverse_transform(lst_output)
k

#Merge the the past data and next 10 days output prediction.
df3=data_oil.tolist()
df3.extend(lst_output)
plt.plot(df3[16400:])

#Reversing the predictions.
df3=scaler.inverse_transform(df3).tolist()

